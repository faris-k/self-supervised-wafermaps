{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings of All Models\n",
    "Here we create UMAP and DensMAP embeddings out of model predictions. Each model performs inference on the training data `train_data` and a subset of the training data, `train_20_split`. Figure outputs can be found at https://github.com/faris-k/self-supervised-wafermaps/tree/master/reports/figures/UMAP. The cell outputs have been cleared to reduce file size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import umap\n",
    "from lightly.data import LightlyDataset\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ssl_wafermap.data import WaferMapDataset\n",
    "from ssl_wafermap.models.knn import (\n",
    "    BYOL,\n",
    "    DCLW,\n",
    "    DINO,\n",
    "    MAE,\n",
    "    MSN,\n",
    "    PMSN,\n",
    "    BarlowTwins,\n",
    "    DINOViT,\n",
    "    FastSiam,\n",
    "    MoCo,\n",
    "    SimCLR,\n",
    "    SimMIM,\n",
    "    SimSiam,\n",
    "    SwaV,\n",
    "    VICReg,\n",
    ")\n",
    "from ssl_wafermap.transforms import get_inference_transforms\n",
    "from ssl_wafermap.utilities.plotting import init_seaborn_style, matplotlibify\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "init_seaborn_style()\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ignore pytorch lightning warning about dataloader workers\n",
    "warnings.filterwarnings(\"ignore\", message=\".*does not have many workers.*\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forgot to capitalize `\"none\"` to `\"None\"` in the preprocessing notebook. Instead of messing with it (since it involves randomly splitting the dataset), I just change this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No additional preprocessing needed for test_data.pkl.xz\n",
      "No additional preprocessing needed for train_10_split.pkl.xz\n",
      "No additional preprocessing needed for train_1_split.pkl.xz\n",
      "No additional preprocessing needed for train_20_split.pkl.xz\n",
      "No additional preprocessing needed for train_29_split.pkl.xz\n",
      "No additional preprocessing needed for train_data.pkl.xz\n",
      "No additional preprocessing needed for train_val_data.pkl.xz\n",
      "No additional preprocessing needed for val_data.pkl.xz\n"
     ]
    }
   ],
   "source": [
    "# Mapping for sorting the failureType columns for better visualizations\n",
    "mapping = {\n",
    "    \"Loc\": 0,\n",
    "    \"None\": 1,\n",
    "    \"Scratch\": 2,\n",
    "    \"Edge-Ring\": 3,\n",
    "    \"Center\": 4,\n",
    "    \"Edge-Loc\": 5,\n",
    "    \"Random\": 6,\n",
    "    \"Donut\": 7,\n",
    "    \"Near-full\": 8,\n",
    "}\n",
    "\n",
    "data_dir = \"../data/processed/WM811K/\"\n",
    "for file in os.listdir(data_dir):\n",
    "    filepath = os.path.join(data_dir, file)\n",
    "    df = pd.read_pickle(filepath)\n",
    "    if \"none\" in df[\"failureType\"].unique():\n",
    "        # If the failureType column has lowercase \"none\", change it to \"None\"\n",
    "        df[\"failureType\"] = df[\"failureType\"].str.replace(\"none\", \"None\")\n",
    "        # Now, convert the failureType column to a categorical column\n",
    "        df[\"failureType\"] = pd.Categorical(\n",
    "            df[\"failureType\"], categories=mapping.keys(), ordered=True\n",
    "        )\n",
    "        df.sort_values(by=\"failureType\", inplace=True)\n",
    "        display(df)\n",
    "        # Save the updated dataframe if need be\n",
    "        print(f\"Saving updated dataframe for {file}\")\n",
    "        df.to_pickle(filepath, compression=\"xz\")\n",
    "    else:\n",
    "        print(f\"No additional preprocessing needed for {file}\")\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_pickle(\"../data/processed/WM811K/train_data.pkl.xz\")\n",
    "df_subset = pd.read_pickle(\"../data/processed/WM811K/train_20_split.pkl.xz\")\n",
    "\n",
    "full_dataset = LightlyDataset.from_torch_dataset(\n",
    "    WaferMapDataset(df_full.waferMap, df_full.failureCode),\n",
    "    transform=get_inference_transforms(),\n",
    ")\n",
    "subset_dataset = LightlyDataset.from_torch_dataset(\n",
    "    WaferMapDataset(df_subset.waferMap, df_subset.failureCode),\n",
    "    transform=get_inference_transforms(),\n",
    ")\n",
    "\n",
    "full_loader = DataLoader(full_dataset, batch_size=1024, shuffle=False)\n",
    "subset_loader = DataLoader(subset_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = \"../models/new_knn/\"\n",
    "ckpt_file_end = \"checkpoints/epoch=149-step=87450.ckpt\"\n",
    "\n",
    "save_path_full = \"../reports/figures/UMAP/full\"\n",
    "save_path_subset = \"../reports/figures/UMAP/subset\"\n",
    "\n",
    "os.makedirs(save_path_full, exist_ok=True)\n",
    "os.makedirs(save_path_subset, exist_ok=True)\n",
    "\n",
    "for folder in os.listdir(ckpt_dir):\n",
    "    # Full path, i.e. ../models/new_knn/MAE/checkpoints/epoch=149-step=87450.ckpt\n",
    "    ckpt_path = os.path.join(ckpt_dir, folder, ckpt_file_end)\n",
    "\n",
    "    # Get model name, i.e. MAE\n",
    "    model_name = ckpt_path.split(\"new_knn/\")[-1].split(\"\\\\\")[0]\n",
    "    print(model_name)\n",
    "    if model_name == \"MAE2\":\n",
    "        model_name = \"MAE\"\n",
    "    # Get the model class using the model_name\n",
    "    ModelClass = getattr(sys.modules[__name__], model_name)\n",
    "\n",
    "    # Instantiate a model of this class and load ckpt weights\n",
    "    model = ModelClass().load_from_checkpoint(ckpt_path)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        logger=False,\n",
    "        inference_mode=True,\n",
    "        precision=\"16-mixed\",\n",
    "        enable_progress_bar=False,\n",
    "    )\n",
    "\n",
    "    # For each dataloader,\n",
    "    for loader, df in zip([subset_loader, full_loader], [df_subset, df_full]):\n",
    "        # for loader, df in zip([full_loader, subset_loader], [df_full, df_subset]):\n",
    "        # Perform inference\n",
    "        preds = trainer.predict(model, loader)\n",
    "        preds = torch.cat(preds).cpu().numpy()\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        preds = scaler.fit_transform(preds)\n",
    "\n",
    "        # Create a UMAP embedding\n",
    "        reducer = umap.UMAP(random_state=0)\n",
    "        embeddings = reducer.fit_transform(preds)\n",
    "        umap_df = pd.DataFrame(embeddings, columns=[\"umap_x\", \"umap_y\"])\n",
    "\n",
    "        # Create a DensMAP embedding\n",
    "        reducer = umap.UMAP(random_state=0, densmap=True, dens_lambda=1)\n",
    "        dense_embeddings = reducer.fit_transform(preds)\n",
    "        densmap_df = pd.DataFrame(dense_embeddings, columns=[\"densmap_x\", \"densmap_y\"])\n",
    "\n",
    "        # Create a dataframe with the embeddings and the failureType\n",
    "        emb_df = pd.concat([umap_df, densmap_df], axis=1)\n",
    "        emb_df[\"failureType\"] = df.failureType.values\n",
    "\n",
    "        # Plot the embeddings side-by-side\n",
    "\n",
    "        # Initialize subplots\n",
    "        fig = make_subplots(\n",
    "            rows=1,\n",
    "            cols=2,\n",
    "            subplot_titles=(f\"{model_name} UMAP\", f\"{model_name} DensMAP\"),\n",
    "            horizontal_spacing=0.01,\n",
    "        )\n",
    "\n",
    "        # Initialize individual figures\n",
    "        fig1 = px.scatter(\n",
    "            emb_df,\n",
    "            x=\"umap_x\",\n",
    "            y=\"umap_y\",\n",
    "            color=\"failureType\",\n",
    "        )\n",
    "        fig2 = px.scatter(\n",
    "            emb_df,\n",
    "            x=\"densmap_x\",\n",
    "            y=\"densmap_y\",\n",
    "            color=\"failureType\",\n",
    "        )\n",
    "\n",
    "        # To prevent the legend from showing up twice, disable it for the second plot\n",
    "        for trace in fig2.data:\n",
    "            trace.update(showlegend=False)\n",
    "        # Now, add the traces to the figure\n",
    "        for trace1, trace2 in zip(fig1.data, fig2.data):\n",
    "            fig.add_trace(trace1, row=1, col=1)\n",
    "            fig.add_trace(trace2, row=1, col=2)\n",
    "        # Update the layout\n",
    "        fig.update_layout(\n",
    "            height=600,\n",
    "            width=1500,\n",
    "            legend_title=\"Failure Type\",\n",
    "            margin=dict(r=200, t=40, b=0, l=0),\n",
    "            legend={\"itemsizing\": \"constant\"},\n",
    "            font=dict(family=\"Arial\", size=24),\n",
    "            xaxis_title=\"\",\n",
    "            yaxis_title=\"\",\n",
    "            template=\"simple_white\",\n",
    "        )\n",
    "        # Increase font size of subplot titles\n",
    "        fig.for_each_annotation(lambda a: a.update(font=dict(family=\"Arial\", size=26)))\n",
    "        fig.update_xaxes(\n",
    "            showgrid=False,\n",
    "            showticklabels=False,\n",
    "            ticks=\"\",\n",
    "            zeroline=False,\n",
    "            showline=True,\n",
    "            linewidth=2.4,\n",
    "            linecolor=\"black\",\n",
    "            mirror=\"allticks\",\n",
    "        )\n",
    "        fig.update_yaxes(\n",
    "            showgrid=False,\n",
    "            showticklabels=False,\n",
    "            ticks=\"\",\n",
    "            zeroline=False,\n",
    "            showline=True,\n",
    "            linewidth=2.4,\n",
    "            linecolor=\"black\",\n",
    "            mirror=\"allticks\",\n",
    "        )\n",
    "        # If we are plotting on the full dataset, use smaller markers\n",
    "        if loader == full_loader:\n",
    "            fig.update_traces(\n",
    "                marker=dict(\n",
    "                    size=4,\n",
    "                ),\n",
    "            )\n",
    "            fig.write_image(f\"{save_path_full}/{model_name}-full.png\", scale=3)\n",
    "        else:\n",
    "            fig.write_image(f\"{save_path_subset}/{model_name}-sub.png\", scale=3)\n",
    "        fig.show(\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = \"../models/new_knn/\"\n",
    "ckpt_file_end = \"checkpoints/epoch=149-step=87450.ckpt\"\n",
    "\n",
    "save_path_full = \"../reports/figures/UMAP/full\"\n",
    "save_path_subset = \"../reports/figures/UMAP/subset\"\n",
    "\n",
    "os.makedirs(save_path_full, exist_ok=True)\n",
    "os.makedirs(save_path_subset, exist_ok=True)\n",
    "\n",
    "for folder in os.listdir(ckpt_dir):\n",
    "    # Full path, i.e. ../models/new_knn/MAE/checkpoints/epoch=149-step=87450.ckpt\n",
    "    ckpt_path = os.path.join(ckpt_dir, folder, ckpt_file_end)\n",
    "\n",
    "    # Get model name, i.e. MAE\n",
    "    model_name = ckpt_path.split(\"new_knn/\")[-1].split(\"\\\\\")[0]\n",
    "    print(model_name)\n",
    "    if model_name == \"MAE2\":\n",
    "        model_name = \"MAE\"\n",
    "    # Get the model class using the model_name\n",
    "    ModelClass = getattr(sys.modules[__name__], model_name)\n",
    "\n",
    "    # Instantiate a model of this class and load ckpt weights\n",
    "    model = ModelClass().load_from_checkpoint(ckpt_path)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        logger=False,\n",
    "        inference_mode=True,\n",
    "        precision=\"16-mixed\",\n",
    "        enable_progress_bar=False,\n",
    "    )\n",
    "\n",
    "    \n",
    "    # For each dataloader,\n",
    "    for loader, df in zip([subset_loader, full_loader], [df_subset, df_full]):\n",
    "        # Perform inference\n",
    "        preds = trainer.predict(model, loader)\n",
    "        preds = torch.cat(preds).cpu().numpy()\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        preds = scaler.fit_transform(preds)\n",
    "        \n",
    "        for label_frac in [0.1, 0.25, 0.5, 0.75, 0.99]:\n",
    "            # We want to give UMAP 10% of the labels for supervised dimensionality reduction\n",
    "            # We will use the same labels for both UMAP and DensMAP\n",
    "            # Select a stratified sample of 10% of the labels from df (df.failureCode)\n",
    "            # Everything that isn't selected, set to -1\n",
    "            labels = df.failureCode.copy()\n",
    "            train, test = train_test_split(labels, train_size=label_frac, random_state=42, stratify=labels)\n",
    "\n",
    "            # Everything in the test set is now labeled as -1\n",
    "            test[:] = -1\n",
    "\n",
    "            # Now concatenate the train and test sets and reindex such that it matches the original dataframe\n",
    "            labels = pd.concat([train, test]).reindex_like(labels)\n",
    "            \n",
    "            # Create a UMAP embedding\n",
    "            reducer = umap.UMAP(random_state=0)\n",
    "            reducer.fit(preds, y=labels)\n",
    "            embeddings = reducer.transform(preds)\n",
    "            umap_df = pd.DataFrame(embeddings, columns=[\"umap_x\", \"umap_y\"])\n",
    "\n",
    "            # Create a DensMAP embedding\n",
    "            reducer = umap.UMAP(random_state=0, densmap=True, dens_lambda=1)\n",
    "            reducer.fit(preds, y=labels)\n",
    "            dense_embeddings = reducer.transform(preds)\n",
    "            densmap_df = pd.DataFrame(dense_embeddings, columns=[\"densmap_x\", \"densmap_y\"])\n",
    "\n",
    "            # Create a dataframe with the embeddings and the failureType\n",
    "            emb_df = pd.concat([umap_df, densmap_df], axis=1)\n",
    "            emb_df[\"failureType\"] = df.failureType.values\n",
    "\n",
    "            # Plot the embeddings side-by-side\n",
    "\n",
    "            # Initialize subplots\n",
    "            fig = make_subplots(\n",
    "                rows=1,\n",
    "                cols=2,\n",
    "                subplot_titles=(f\"{model_name} Semi-Supervised UMAP\", f\"{model_name} Semi-Supervised DensMAP\"),\n",
    "                horizontal_spacing=0.01,\n",
    "            )\n",
    "\n",
    "            # Initialize individual figures\n",
    "            fig1 = px.scatter(\n",
    "                emb_df,\n",
    "                x=\"umap_x\",\n",
    "                y=\"umap_y\",\n",
    "                color=\"failureType\",\n",
    "            )\n",
    "            fig2 = px.scatter(\n",
    "                emb_df,\n",
    "                x=\"densmap_x\",\n",
    "                y=\"densmap_y\",\n",
    "                color=\"failureType\",\n",
    "            )\n",
    "\n",
    "            # To prevent the legend from showing up twice, disable it for the second plot\n",
    "            for trace in fig2.data:\n",
    "                trace.update(showlegend=False)\n",
    "            # Now, add the traces to the figure\n",
    "            for trace1, trace2 in zip(fig1.data, fig2.data):\n",
    "                fig.add_trace(trace1, row=1, col=1)\n",
    "                fig.add_trace(trace2, row=1, col=2)\n",
    "            # Update the layout\n",
    "            fig.update_layout(\n",
    "                height=600,\n",
    "                width=1500,\n",
    "                legend_title=\"Failure Type\",\n",
    "                margin=dict(r=200, t=40, b=0, l=0),\n",
    "                legend={\"itemsizing\": \"constant\"},\n",
    "                font=dict(family=\"Arial\", size=24),\n",
    "                xaxis_title=\"\",\n",
    "                yaxis_title=\"\",\n",
    "                template=\"simple_white\",\n",
    "            )\n",
    "            # Increase font size of subplot titles\n",
    "            fig.for_each_annotation(lambda a: a.update(font=dict(family=\"Arial\", size=26)))\n",
    "            fig.update_xaxes(\n",
    "                showgrid=False,\n",
    "                showticklabels=False,\n",
    "                ticks=\"\",\n",
    "                zeroline=False,\n",
    "                showline=True,\n",
    "                linewidth=2.4,\n",
    "                linecolor=\"black\",\n",
    "                mirror=\"allticks\",\n",
    "            )\n",
    "            fig.update_yaxes(\n",
    "                showgrid=False,\n",
    "                showticklabels=False,\n",
    "                ticks=\"\",\n",
    "                zeroline=False,\n",
    "                showline=True,\n",
    "                linewidth=2.4,\n",
    "                linecolor=\"black\",\n",
    "                mirror=\"allticks\",\n",
    "            )\n",
    "            # If we are plotting on the full dataset, use smaller markers\n",
    "            if loader == full_loader:\n",
    "                fig.update_traces(\n",
    "                    marker=dict(\n",
    "                        size=4,\n",
    "                    ),\n",
    "                )\n",
    "                fig.write_image(f\"{save_path_full}/{model_name}-full-{label_frac}.png\", scale=3)\n",
    "            else:\n",
    "                fig.write_image(f\"{save_path_subset}/{model_name}-sub-{label_frac}.png\", scale=3)\n",
    "            fig.show(\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/interim/model_preds\", exist_ok=True)\n",
    "\n",
    "for folder in os.listdir(ckpt_dir):\n",
    "    # Full path, i.e. ../models/new_knn/MAE/checkpoints/epoch=149-step=87450.ckpt\n",
    "    ckpt_path = os.path.join(ckpt_dir, folder, ckpt_file_end)\n",
    "\n",
    "    # Get model name, i.e. MAE\n",
    "    model_name = ckpt_path.split(\"new_knn/\")[-1].split(\"\\\\\")[0]\n",
    "    print(model_name)\n",
    "    if model_name == \"MAE2\":\n",
    "        model_name = \"MAE\"\n",
    "    # Get the model class using the model_name\n",
    "    ModelClass = getattr(sys.modules[__name__], model_name)\n",
    "\n",
    "    # Instantiate a model of this class and load ckpt weights\n",
    "    model = ModelClass().load_from_checkpoint(ckpt_path)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        logger=False,\n",
    "        inference_mode=True,\n",
    "        precision=\"16-mixed\",\n",
    "        enable_progress_bar=False,\n",
    "    )\n",
    "\n",
    "    \n",
    "    # For each dataloader,\n",
    "    for loader, df in zip([subset_loader, full_loader], [df_subset, df_full]):\n",
    "        # Perform inference\n",
    "        preds = trainer.predict(model, loader)\n",
    "        preds = torch.cat(preds).cpu().numpy()\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        preds = scaler.fit_transform(preds)\n",
    "        \n",
    "        preds = pd.DataFrame(preds)\n",
    "        preds[\"waferMap\"] = df.waferMap.values\n",
    "        preds[\"failureType\"] = df.failureType.values\n",
    "        preds[\"failureCode\"] = df.failureCode.values\n",
    "\n",
    "        if loader == full_loader:\n",
    "            preds.to_pickle(\n",
    "                f\"../data/interim/model_preds/{model_name}_preds_full.pkl.xz\", compression=\"xz\"\n",
    "            )\n",
    "        else:\n",
    "            preds.to_pickle(\n",
    "                f\"../data/interim/model_preds/{model_name}_preds_subset.pkl.xz\", compression=\"xz\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_ckpt = \"D:\\\\Documents\\\\GitHub\\\\fastsiam-wafers\\\\scripts\\\\benchmark_logs\\\\wafermaps\\\\version_34\\\\FastSiamSymmetrized\\\\checkpoints\\\\epoch=199-step=31000.ckpt\"\n",
    "model = FastSiam().load_from_checkpoint(bad_ckpt)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    logger=False,\n",
    "    inference_mode=True,\n",
    "    precision=\"16-mixed\",\n",
    "    enable_progress_bar=False,\n",
    ")\n",
    "\n",
    "preds = trainer.predict(model, subset_loader)\n",
    "preds = torch.cat(preds).cpu().numpy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "preds = scaler.fit_transform(preds)\n",
    "\n",
    "# Create a UMAP embedding\n",
    "reducer = umap.UMAP(random_state=0)\n",
    "embeddings = reducer.fit_transform(preds)\n",
    "umap_df = pd.DataFrame(embeddings, columns=[\"umap_x\", \"umap_y\"])\n",
    "\n",
    "# Create a DensMAP embedding\n",
    "reducer = umap.UMAP(random_state=0, densmap=True, dens_lambda=1)\n",
    "dense_embeddings = reducer.fit_transform(preds)\n",
    "densmap_df = pd.DataFrame(dense_embeddings, columns=[\"densmap_x\", \"densmap_y\"])\n",
    "\n",
    "# Create a dataframe with the embeddings and the failureType\n",
    "emb_df = pd.concat([umap_df, densmap_df], axis=1)\n",
    "emb_df[\"failureType\"] = df_subset.failureType.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the embeddings side-by-side\n",
    "\n",
    "# Initialize subplots\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(f\"FastSiam UMAP\", f\"FastSiam DensMAP\"),\n",
    "    horizontal_spacing=0.01,\n",
    ")\n",
    "\n",
    "# Initialize individual figures\n",
    "fig1 = px.scatter(\n",
    "    emb_df,\n",
    "    x=\"umap_x\",\n",
    "    y=\"umap_y\",\n",
    "    color=\"failureType\",\n",
    ")\n",
    "fig2 = px.scatter(\n",
    "    emb_df,\n",
    "    x=\"densmap_x\",\n",
    "    y=\"densmap_y\",\n",
    "    color=\"failureType\",\n",
    ")\n",
    "\n",
    "# To prevent the legend from showing up twice, disable it for the second plot\n",
    "for trace in fig2.data:\n",
    "    trace.update(showlegend=False)\n",
    "# Now, add the traces to the figure\n",
    "for trace1, trace2 in zip(fig1.data, fig2.data):\n",
    "    fig.add_trace(trace1, row=1, col=1)\n",
    "    fig.add_trace(trace2, row=1, col=2)\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    width=1500,\n",
    "    legend_title=\"Failure Type\",\n",
    "    margin=dict(r=200, t=40, b=0, l=0),\n",
    "    legend={\"itemsizing\": \"constant\"},\n",
    "    font=dict(family=\"Arial\", size=24),\n",
    "    xaxis_title=\"\",\n",
    "    yaxis_title=\"\",\n",
    "    template=\"simple_white\",\n",
    ")\n",
    "# Increase font size of subplot titles\n",
    "fig.for_each_annotation(lambda a: a.update(font=dict(family=\"Arial\", size=26)))\n",
    "fig.update_xaxes(\n",
    "    showgrid=False,\n",
    "    showticklabels=False,\n",
    "    ticks=\"\",\n",
    "    zeroline=False,\n",
    "    showline=True,\n",
    "    linewidth=2.4,\n",
    "    linecolor=\"black\",\n",
    "    mirror=\"allticks\",\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    showgrid=False,\n",
    "    showticklabels=False,\n",
    "    ticks=\"\",\n",
    "    zeroline=False,\n",
    "    showline=True,\n",
    "    linewidth=2.4,\n",
    "    linecolor=\"black\",\n",
    "    mirror=\"allticks\",\n",
    ")\n",
    "fig.write_image(f\"{save_path_subset}/FastSiam-collapse-old-sub.png\", scale=3)\n",
    "fig.show(\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.density_contour(\n",
    "    emb_df,\n",
    "    x=\"umap_x\",\n",
    "    y=\"umap_y\",\n",
    "    color=\"failureType\",\n",
    "    marginal_x=\"box\",\n",
    "    marginal_y=\"violin\",\n",
    "    width=800,\n",
    "    height=600,\n",
    ")\n",
    "fig.update_layout(\n",
    "    legend={\"itemsizing\": \"constant\"},\n",
    ")\n",
    "\n",
    "fig.show(\"svg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wafer-ssl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
